{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 경고 뜨지 않게 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 데이터 전처리 알고리즘\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습용과 검증용으로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 교차 검증\n",
    "# 지표를 하나만 설정할 경우\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 지표를 하나 이상 설정할 경우\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 모델의 최적의 하이퍼파라미터를 찾기 위한 도구\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 평가함수\n",
    "# 분류용\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 회귀용\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 군집화\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "\n",
    "\n",
    "\n",
    "# ARIMA (시계열 예측)\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# 시간 측정을 위한 시간 모듈\n",
    "import datetime\n",
    "\n",
    "# 주식정보\n",
    "from pandas_datareader import data\n",
    "\n",
    "# 형태소 벡터를 생성하기 위한 라이브러리\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 형태소 벡터를 학습 벡터로 변환한다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# 데이터 수집\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 한국어 형태소 분석\n",
    "from konlpy.tag import Okt, Hannanum, Kkma, Mecab, Komoran\n",
    "\n",
    "# 워드 클라우드를 위한 라이브러리\n",
    "from collections import Counter\n",
    "import pytagcloud\n",
    "from IPython.display import Image\n",
    "\n",
    "# 출력창 청소를 위한 함수\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "\n",
    "# 딥러닝 모델 구조를 정의하는 것\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 층구조를 정의하는 것\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 활성화 함수를 정의하는 것\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# 다중 분류를 위한 원핫 인코딩\n",
    "# 결과 데이터의 종류 수 만큼 결과 데이터의 칼럼을 늘리는 작업\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 현재 프로젝트를 gpu에 할당한다.\n",
    "# 컴퓨터의 GPU는 메모리를 가지고 있다. \n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# gpu가 있다면\n",
    "if len(gpus) >0 :\n",
    "    try :\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e :\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 설정\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/sonar.csv\", header=None)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop(60, axis=1)\n",
    "y = df1[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1= LabelEncoder()\n",
    "encoder1.fit(y)\n",
    "y = encoder1.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비율 설정\n",
    "- 주어진 데이터를 랜덤하게 섞은 후 주어진 비율 만큼 학습셋과 검증셋을 나눠 학습과 평가를 실시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 2s 35ms/step - loss: 0.6620 - accuracy: 0.6828 - val_loss: 0.8865 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6348 - accuracy: 0.6966 - val_loss: 0.9761 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6230 - accuracy: 0.7103 - val_loss: 1.0030 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.6112 - accuracy: 0.7241 - val_loss: 1.0499 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5993 - accuracy: 0.7172 - val_loss: 1.1585 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.5870 - accuracy: 0.7103 - val_loss: 1.1761 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5726 - accuracy: 0.7103 - val_loss: 1.2233 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.5626 - accuracy: 0.7310 - val_loss: 1.2507 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.5491 - accuracy: 0.7586 - val_loss: 1.2265 - val_accuracy: 0.0159\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.5375 - accuracy: 0.7655 - val_loss: 1.2786 - val_accuracy: 0.0159\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5302 - accuracy: 0.7517 - val_loss: 1.1943 - val_accuracy: 0.0476\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.5178 - accuracy: 0.7793 - val_loss: 1.3536 - val_accuracy: 0.0159\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5056 - accuracy: 0.7586 - val_loss: 1.2836 - val_accuracy: 0.0476\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4933 - accuracy: 0.7724 - val_loss: 1.3360 - val_accuracy: 0.0476\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4831 - accuracy: 0.7724 - val_loss: 1.4248 - val_accuracy: 0.0317\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4733 - accuracy: 0.7655 - val_loss: 1.2988 - val_accuracy: 0.0952\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4617 - accuracy: 0.7724 - val_loss: 1.4019 - val_accuracy: 0.0635\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4532 - accuracy: 0.7793 - val_loss: 1.4311 - val_accuracy: 0.0794\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4380 - accuracy: 0.8000 - val_loss: 1.4463 - val_accuracy: 0.1111\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4323 - accuracy: 0.8069 - val_loss: 1.4305 - val_accuracy: 0.1587\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4231 - accuracy: 0.8000 - val_loss: 1.3784 - val_accuracy: 0.1746\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4091 - accuracy: 0.8345 - val_loss: 1.4762 - val_accuracy: 0.1746\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3978 - accuracy: 0.8069 - val_loss: 1.5883 - val_accuracy: 0.1746\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3891 - accuracy: 0.8138 - val_loss: 1.4241 - val_accuracy: 0.2540\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.3797 - accuracy: 0.8345 - val_loss: 1.5279 - val_accuracy: 0.2381\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.3721 - accuracy: 0.8276 - val_loss: 1.5366 - val_accuracy: 0.2381\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3643 - accuracy: 0.8414 - val_loss: 1.4864 - val_accuracy: 0.2857\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3609 - accuracy: 0.8345 - val_loss: 1.5987 - val_accuracy: 0.2540\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3512 - accuracy: 0.8552 - val_loss: 1.5142 - val_accuracy: 0.3016\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3553 - accuracy: 0.8345 - val_loss: 1.5875 - val_accuracy: 0.3016\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3335 - accuracy: 0.8621 - val_loss: 1.4863 - val_accuracy: 0.3175\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3303 - accuracy: 0.8621 - val_loss: 1.6631 - val_accuracy: 0.2857\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3229 - accuracy: 0.8621 - val_loss: 1.6588 - val_accuracy: 0.3016\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3223 - accuracy: 0.8621 - val_loss: 1.6702 - val_accuracy: 0.3016\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3158 - accuracy: 0.8690 - val_loss: 1.6897 - val_accuracy: 0.3016\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3030 - accuracy: 0.8759 - val_loss: 1.6237 - val_accuracy: 0.3016\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.2987 - accuracy: 0.8759 - val_loss: 1.7165 - val_accuracy: 0.3016\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2994 - accuracy: 0.8897 - val_loss: 1.6100 - val_accuracy: 0.3016\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.2999 - accuracy: 0.8621 - val_loss: 1.6712 - val_accuracy: 0.3016\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.2838 - accuracy: 0.8828 - val_loss: 1.7254 - val_accuracy: 0.3016\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2796 - accuracy: 0.8897 - val_loss: 1.7374 - val_accuracy: 0.3016\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2755 - accuracy: 0.8966 - val_loss: 1.6816 - val_accuracy: 0.3016\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2685 - accuracy: 0.9034 - val_loss: 1.7565 - val_accuracy: 0.3016\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2642 - accuracy: 0.8828 - val_loss: 1.8532 - val_accuracy: 0.3016\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.2643 - accuracy: 0.9034 - val_loss: 1.8008 - val_accuracy: 0.3016\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2618 - accuracy: 0.8828 - val_loss: 1.7982 - val_accuracy: 0.3016\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2548 - accuracy: 0.9103 - val_loss: 1.6868 - val_accuracy: 0.3175\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2461 - accuracy: 0.9310 - val_loss: 1.9252 - val_accuracy: 0.3016\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2442 - accuracy: 0.9034 - val_loss: 1.7740 - val_accuracy: 0.3175\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.2435 - accuracy: 0.9172 - val_loss: 1.6883 - val_accuracy: 0.3175\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2377 - accuracy: 0.9172 - val_loss: 1.8693 - val_accuracy: 0.3175\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2395 - accuracy: 0.9034 - val_loss: 1.7567 - val_accuracy: 0.3175\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2359 - accuracy: 0.9310 - val_loss: 1.8995 - val_accuracy: 0.3175\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2273 - accuracy: 0.9241 - val_loss: 1.7854 - val_accuracy: 0.3175\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2224 - accuracy: 0.9310 - val_loss: 1.9062 - val_accuracy: 0.3175\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2157 - accuracy: 0.9448 - val_loss: 1.8760 - val_accuracy: 0.3175\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2136 - accuracy: 0.9379 - val_loss: 1.8156 - val_accuracy: 0.3492\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2061 - accuracy: 0.9448 - val_loss: 2.0269 - val_accuracy: 0.2698\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2058 - accuracy: 0.9448 - val_loss: 2.0080 - val_accuracy: 0.3175\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2008 - accuracy: 0.9379 - val_loss: 1.8825 - val_accuracy: 0.3333\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.1986 - accuracy: 0.9310 - val_loss: 1.9136 - val_accuracy: 0.3492\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1955 - accuracy: 0.9379 - val_loss: 2.0222 - val_accuracy: 0.3175\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1944 - accuracy: 0.9310 - val_loss: 2.0566 - val_accuracy: 0.3175\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1925 - accuracy: 0.9517 - val_loss: 1.8180 - val_accuracy: 0.3810\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1824 - accuracy: 0.9448 - val_loss: 2.1223 - val_accuracy: 0.3175\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1780 - accuracy: 0.9517 - val_loss: 2.0070 - val_accuracy: 0.3492\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1830 - accuracy: 0.9310 - val_loss: 1.9716 - val_accuracy: 0.3333\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.1765 - accuracy: 0.9379 - val_loss: 2.0509 - val_accuracy: 0.3492\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.1687 - accuracy: 0.9517 - val_loss: 2.2397 - val_accuracy: 0.2857\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.1677 - accuracy: 0.9517 - val_loss: 1.7513 - val_accuracy: 0.3968\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1622 - accuracy: 0.9655 - val_loss: 2.2598 - val_accuracy: 0.3016\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1550 - accuracy: 0.9517 - val_loss: 1.9966 - val_accuracy: 0.3651\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.1523 - accuracy: 0.9724 - val_loss: 2.0266 - val_accuracy: 0.3651\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1522 - accuracy: 0.9586 - val_loss: 2.0620 - val_accuracy: 0.3651\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1475 - accuracy: 0.9655 - val_loss: 2.0526 - val_accuracy: 0.3492\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1443 - accuracy: 0.9724 - val_loss: 2.1949 - val_accuracy: 0.3175\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1373 - accuracy: 0.9586 - val_loss: 2.2083 - val_accuracy: 0.3333\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1366 - accuracy: 0.9724 - val_loss: 2.1139 - val_accuracy: 0.3651\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1318 - accuracy: 0.9724 - val_loss: 2.1340 - val_accuracy: 0.3333\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1306 - accuracy: 0.9655 - val_loss: 2.1366 - val_accuracy: 0.3175\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1291 - accuracy: 0.9724 - val_loss: 2.3071 - val_accuracy: 0.3333\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1258 - accuracy: 0.9793 - val_loss: 2.0785 - val_accuracy: 0.3016\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1240 - accuracy: 0.9724 - val_loss: 2.1805 - val_accuracy: 0.3492\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1211 - accuracy: 0.9793 - val_loss: 2.2105 - val_accuracy: 0.3651\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1141 - accuracy: 0.9862 - val_loss: 2.2532 - val_accuracy: 0.3016\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1113 - accuracy: 0.9724 - val_loss: 2.2146 - val_accuracy: 0.3492\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.1130 - accuracy: 0.9793 - val_loss: 2.3807 - val_accuracy: 0.3175\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1067 - accuracy: 0.9862 - val_loss: 2.4133 - val_accuracy: 0.3175\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1057 - accuracy: 0.9862 - val_loss: 2.2302 - val_accuracy: 0.3492\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1051 - accuracy: 0.9793 - val_loss: 2.6307 - val_accuracy: 0.3016\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0986 - accuracy: 0.9793 - val_loss: 2.2714 - val_accuracy: 0.3492\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0995 - accuracy: 0.9862 - val_loss: 2.4805 - val_accuracy: 0.3175\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0956 - accuracy: 0.9862 - val_loss: 2.4080 - val_accuracy: 0.3175\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0937 - accuracy: 0.9862 - val_loss: 2.4374 - val_accuracy: 0.3492\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0926 - accuracy: 0.9724 - val_loss: 2.4259 - val_accuracy: 0.3175\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0918 - accuracy: 0.9862 - val_loss: 2.4445 - val_accuracy: 0.3175\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0882 - accuracy: 0.9862 - val_loss: 2.5965 - val_accuracy: 0.3175\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0832 - accuracy: 0.9862 - val_loss: 2.5399 - val_accuracy: 0.3175\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0841 - accuracy: 0.9862 - val_loss: 2.3358 - val_accuracy: 0.3651\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0830 - accuracy: 0.9862 - val_loss: 2.4295 - val_accuracy: 0.3651\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0758 - accuracy: 0.9862 - val_loss: 2.6207 - val_accuracy: 0.3175\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0764 - accuracy: 0.9862 - val_loss: 2.4559 - val_accuracy: 0.3651\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0740 - accuracy: 0.9862 - val_loss: 2.4982 - val_accuracy: 0.3333\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0727 - accuracy: 0.9862 - val_loss: 2.8082 - val_accuracy: 0.3175\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0695 - accuracy: 0.9862 - val_loss: 2.6947 - val_accuracy: 0.3175\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0685 - accuracy: 0.9931 - val_loss: 2.5517 - val_accuracy: 0.3492\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0664 - accuracy: 0.9862 - val_loss: 2.7494 - val_accuracy: 0.3175\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 2.8297 - val_accuracy: 0.3175\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0624 - accuracy: 0.9862 - val_loss: 2.6893 - val_accuracy: 0.3492\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9862 - val_loss: 2.9240 - val_accuracy: 0.3175\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 0.9862 - val_loss: 2.8761 - val_accuracy: 0.3175\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0613 - accuracy: 0.9931 - val_loss: 2.6424 - val_accuracy: 0.3651\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0550 - accuracy: 0.9931 - val_loss: 3.0336 - val_accuracy: 0.3175\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 2.6924 - val_accuracy: 0.3492\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 2.8095 - val_accuracy: 0.3333\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 3.0441 - val_accuracy: 0.3175\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 2.7249 - val_accuracy: 0.3651\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 3.0025 - val_accuracy: 0.3175\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0494 - accuracy: 0.9931 - val_loss: 2.9333 - val_accuracy: 0.3333\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 2.8940 - val_accuracy: 0.3333\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 3.0020 - val_accuracy: 0.3333\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 3.3019 - val_accuracy: 0.3175\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 2.9664 - val_accuracy: 0.3333\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 3.0324 - val_accuracy: 0.3492\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0456 - accuracy: 1.0000 - val_loss: 3.0595 - val_accuracy: 0.3333\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0445 - accuracy: 0.9931 - val_loss: 3.3285 - val_accuracy: 0.3175\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 3.0847 - val_accuracy: 0.3492\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 3.2176 - val_accuracy: 0.3333\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 3.1890 - val_accuracy: 0.3492\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 3.3193 - val_accuracy: 0.3333\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 3.2057 - val_accuracy: 0.3333\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 3.4244 - val_accuracy: 0.3333\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 3.3747 - val_accuracy: 0.3333\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 3.1280 - val_accuracy: 0.3651\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 3.5356 - val_accuracy: 0.3333\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 3.2208 - val_accuracy: 0.3492\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 3.1292 - val_accuracy: 0.3651\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 3.4798 - val_accuracy: 0.3333\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 3.3417 - val_accuracy: 0.3492\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 3.3942 - val_accuracy: 0.3492\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 3.3204 - val_accuracy: 0.3651\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 3.4415 - val_accuracy: 0.3492\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 3.5961 - val_accuracy: 0.3492\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 3.3279 - val_accuracy: 0.3651\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 3.6107 - val_accuracy: 0.3333\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 3.4836 - val_accuracy: 0.3651\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 3.5587 - val_accuracy: 0.3492\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 3.4404 - val_accuracy: 0.3492\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 3.5315 - val_accuracy: 0.3651\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 3.6244 - val_accuracy: 0.3492\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 3.5014 - val_accuracy: 0.3492\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 3.8608 - val_accuracy: 0.3333\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 3.6587 - val_accuracy: 0.3651\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.5952 - val_accuracy: 0.3492\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 3.6327 - val_accuracy: 0.3651\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 3.7718 - val_accuracy: 0.3492\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 3.7861 - val_accuracy: 0.3492\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 3.7157 - val_accuracy: 0.3651\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 3.7595 - val_accuracy: 0.3651\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 3.8904 - val_accuracy: 0.3333\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 3.7944 - val_accuracy: 0.3651\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.8254 - val_accuracy: 0.3651\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 3.7115 - val_accuracy: 0.3651\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 3.8764 - val_accuracy: 0.3651\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 4.0533 - val_accuracy: 0.3175\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 3.7877 - val_accuracy: 0.3651\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.8923 - val_accuracy: 0.3651\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 4.0690 - val_accuracy: 0.3492\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.8305 - val_accuracy: 0.3651\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 3.9487 - val_accuracy: 0.3651\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 3.9790 - val_accuracy: 0.3492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.8106 - val_accuracy: 0.3651\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 4.0093 - val_accuracy: 0.3651\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 3.8789 - val_accuracy: 0.3651\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.8480 - val_accuracy: 0.3651\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 4.1144 - val_accuracy: 0.3651\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 4.0308 - val_accuracy: 0.3651\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.9790 - val_accuracy: 0.3651\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 4.1199 - val_accuracy: 0.3651\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.9423 - val_accuracy: 0.3651\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 4.1390 - val_accuracy: 0.3651\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 4.0493 - val_accuracy: 0.3651\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 4.0669 - val_accuracy: 0.3651\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 4.0615 - val_accuracy: 0.3651\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 4.1559 - val_accuracy: 0.3651\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 4.2105 - val_accuracy: 0.3651\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 4.1631 - val_accuracy: 0.3651\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 4.2356 - val_accuracy: 0.3651\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 4.2260 - val_accuracy: 0.3651\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 4.1766 - val_accuracy: 0.3651\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 4.2366 - val_accuracy: 0.3651\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 4.2830 - val_accuracy: 0.3651\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 4.2224 - val_accuracy: 0.3651\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 4.3798 - val_accuracy: 0.3651\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 4.1126 - val_accuracy: 0.3651\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 4.2789 - val_accuracy: 0.3651\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 4.3405 - val_accuracy: 0.3651\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 4.4915 - val_accuracy: 0.3651\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 4.2081 - val_accuracy: 0.3651\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 4.4199 - val_accuracy: 0.3651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2690c527910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(24, input_dim=60))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 학습한다.\n",
    "# 전체 데이터의 0.3을 비율로 검증용으로 사용한다.\n",
    "model.fit(X, y, epochs=200, batch_size=10, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 1.3440 - accuracy: 0.8077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3440184593200684, 0.807692289352417]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = model.evaluate(X, y)\n",
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss, accuracy : 학습 데이터를 통한 평가\n",
    "- val_loss, val_accuracy : 검증 데이터를 통한 평가\n",
    "- 학습 데이터의 결과는 좋은데 검증 데이터의 결과가 나쁘면 과적합으로 봐야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증용 데이터가 있을 경우\n",
    "- 본 예제에서는 전체 데이터를 학습과 검증용으로 나눈다음 예제를 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.2471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.1107</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1772</td>\n",
       "      <td>0.1908</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.3887</td>\n",
       "      <td>0.7106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.1787</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.2682</td>\n",
       "      <td>0.2058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
       "55   0.0201  0.0116  0.0123  0.0245  0.0547  0.0208  0.0891  0.0836  0.1335   \n",
       "93   0.0459  0.0437  0.0347  0.0456  0.0067  0.0890  0.1798  0.1741  0.1598   \n",
       "53   0.0293  0.0378  0.0257  0.0062  0.0130  0.0612  0.0895  0.1107  0.0973   \n",
       "97   0.0491  0.0279  0.0592  0.1270  0.1772  0.1908  0.2217  0.0768  0.1246   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "137  0.0430  0.0902  0.0833  0.0813  0.0165  0.0277  0.0569  0.2057  0.3887   \n",
       "72   0.0208  0.0186  0.0131  0.0211  0.0610  0.0613  0.0612  0.0506  0.0989   \n",
       "140  0.0412  0.1135  0.0518  0.0232  0.0646  0.1124  0.1787  0.2407  0.2682   \n",
       "37   0.0333  0.0221  0.0270  0.0481  0.0679  0.0981  0.0843  0.1172  0.0759   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "194  0.2471  ...  0.0089  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071   \n",
       "55   0.1199  ...  0.0032  0.0076  0.0045  0.0056  0.0075  0.0037  0.0045   \n",
       "93   0.1408  ...  0.0121  0.0067  0.0032  0.0109  0.0164  0.0151  0.0070   \n",
       "53   0.0751  ...  0.0076  0.0065  0.0072  0.0108  0.0051  0.0102  0.0041   \n",
       "97   0.2028  ...  0.0268  0.0081  0.0129  0.0161  0.0063  0.0119  0.0194   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
       "137  0.7106  ...  0.0208  0.0176  0.0197  0.0210  0.0141  0.0049  0.0027   \n",
       "72   0.1093  ...  0.0140  0.0074  0.0063  0.0081  0.0087  0.0044  0.0028   \n",
       "140  0.2058  ...  0.0798  0.0376  0.0143  0.0272  0.0127  0.0166  0.0095   \n",
       "37   0.0920  ...  0.0036  0.0022  0.0032  0.0060  0.0054  0.0063  0.0143   \n",
       "\n",
       "         57      58      59  \n",
       "194  0.0044  0.0022  0.0014  \n",
       "55   0.0029  0.0008  0.0018  \n",
       "93   0.0085  0.0117  0.0056  \n",
       "53   0.0055  0.0050  0.0087  \n",
       "97   0.0140  0.0332  0.0439  \n",
       "..      ...     ...     ...  \n",
       "203  0.0115  0.0193  0.0157  \n",
       "137  0.0162  0.0059  0.0021  \n",
       "72   0.0019  0.0049  0.0023  \n",
       "140  0.0225  0.0098  0.0085  \n",
       "37   0.0132  0.0051  0.0041  \n",
       "\n",
       "[145 rows x 60 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.1345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2228</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0721</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>0.1626</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0657</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>0.0629</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>0.4223</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0949</td>\n",
       "      <td>0.1504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n",
       "155  0.0211  0.0128  0.0015  0.0450  0.0711  0.1563  0.1518  0.1206  0.1666   \n",
       "165  0.0221  0.0065  0.0164  0.0487  0.0519  0.0849  0.0812  0.1833  0.2228   \n",
       "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n",
       "58   0.0225  0.0019  0.0075  0.0097  0.0445  0.0906  0.0889  0.0655  0.1624   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "88   0.0274  0.0242  0.0621  0.0560  0.1129  0.0973  0.1823  0.1745  0.1440   \n",
       "39   0.0091  0.0213  0.0206  0.0505  0.0657  0.0795  0.0970  0.0872  0.0743   \n",
       "114  0.0114  0.0222  0.0269  0.0384  0.1217  0.2062  0.1489  0.0929  0.1350   \n",
       "132  0.0968  0.0821  0.0629  0.0608  0.0617  0.1207  0.0944  0.4223  0.5744   \n",
       "56   0.0152  0.0102  0.0113  0.0263  0.0097  0.0391  0.0857  0.0915  0.0949   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "186  0.3914  ...  0.0054  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062   \n",
       "155  0.1345  ...  0.0174  0.0117  0.0023  0.0047  0.0049  0.0031  0.0024   \n",
       "165  0.1810  ...  0.0167  0.0089  0.0051  0.0015  0.0075  0.0058  0.0016   \n",
       "200  0.3193  ...  0.0137  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040   \n",
       "58   0.1452  ...  0.0051  0.0034  0.0129  0.0100  0.0044  0.0057  0.0030   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "88   0.1808  ...  0.0255  0.0113  0.0108  0.0085  0.0047  0.0074  0.0104   \n",
       "39   0.0837  ...  0.0300  0.0112  0.0112  0.0102  0.0026  0.0097  0.0098   \n",
       "114  0.1799  ...  0.0213  0.0269  0.0152  0.0257  0.0097  0.0041  0.0050   \n",
       "132  0.5025  ...  0.0206  0.0073  0.0081  0.0303  0.0190  0.0212  0.0126   \n",
       "56   0.1504  ...  0.0048  0.0049  0.0041  0.0036  0.0013  0.0046  0.0037   \n",
       "\n",
       "         57      58      59  \n",
       "186  0.0087  0.0070  0.0042  \n",
       "155  0.0039  0.0051  0.0015  \n",
       "165  0.0070  0.0074  0.0038  \n",
       "200  0.0009  0.0015  0.0085  \n",
       "58   0.0035  0.0021  0.0027  \n",
       "..      ...     ...     ...  \n",
       "88   0.0161  0.0220  0.0173  \n",
       "39   0.0043  0.0071  0.0108  \n",
       "114  0.0145  0.0103  0.0025  \n",
       "132  0.0201  0.0210  0.0041  \n",
       "56   0.0011  0.0034  0.0033  \n",
       "\n",
       "[63 rows x 60 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(24, input_dim=60))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.7546 - accuracy: 0.4690 - val_loss: 0.6749 - val_accuracy: 0.5873\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.6585 - accuracy: 0.6345 - val_loss: 0.6541 - val_accuracy: 0.6032\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.6411 - accuracy: 0.6552 - val_loss: 0.6349 - val_accuracy: 0.6825\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.6245 - accuracy: 0.7034 - val_loss: 0.6343 - val_accuracy: 0.6349\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.6070 - accuracy: 0.7172 - val_loss: 0.6133 - val_accuracy: 0.6984\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.5935 - accuracy: 0.7172 - val_loss: 0.6062 - val_accuracy: 0.7143\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5747 - accuracy: 0.7586 - val_loss: 0.5875 - val_accuracy: 0.7143\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.5597 - accuracy: 0.7448 - val_loss: 0.5957 - val_accuracy: 0.6508\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.5460 - accuracy: 0.7517 - val_loss: 0.5618 - val_accuracy: 0.6984\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.5192 - accuracy: 0.7724 - val_loss: 0.5494 - val_accuracy: 0.7302\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.5055 - accuracy: 0.7724 - val_loss: 0.5345 - val_accuracy: 0.7619\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.4846 - accuracy: 0.7862 - val_loss: 0.5261 - val_accuracy: 0.7143\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.4684 - accuracy: 0.8276 - val_loss: 0.5174 - val_accuracy: 0.6984\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.4623 - accuracy: 0.8069 - val_loss: 0.5113 - val_accuracy: 0.7460\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.4385 - accuracy: 0.8552 - val_loss: 0.5022 - val_accuracy: 0.7460\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.4340 - accuracy: 0.8069 - val_loss: 0.5063 - val_accuracy: 0.7143\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.4114 - accuracy: 0.8414 - val_loss: 0.4981 - val_accuracy: 0.7143\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4020 - accuracy: 0.8966 - val_loss: 0.4877 - val_accuracy: 0.7460\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3866 - accuracy: 0.8690 - val_loss: 0.4839 - val_accuracy: 0.7302\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.3914 - accuracy: 0.8276 - val_loss: 0.4807 - val_accuracy: 0.7143\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.3779 - accuracy: 0.8552 - val_loss: 0.4772 - val_accuracy: 0.7143\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.3542 - accuracy: 0.8690 - val_loss: 0.4731 - val_accuracy: 0.7302\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.3480 - accuracy: 0.8966 - val_loss: 0.4753 - val_accuracy: 0.7460\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3351 - accuracy: 0.9034 - val_loss: 0.4787 - val_accuracy: 0.7143\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3427 - accuracy: 0.8552 - val_loss: 0.4835 - val_accuracy: 0.7619\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.3270 - accuracy: 0.8828 - val_loss: 0.4817 - val_accuracy: 0.7619\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3249 - accuracy: 0.8690 - val_loss: 0.4953 - val_accuracy: 0.7778\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.3146 - accuracy: 0.8828 - val_loss: 0.4648 - val_accuracy: 0.7460\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.3081 - accuracy: 0.8828 - val_loss: 0.4667 - val_accuracy: 0.7302\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.2941 - accuracy: 0.8828 - val_loss: 0.5109 - val_accuracy: 0.7143\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3132 - accuracy: 0.8690 - val_loss: 0.4620 - val_accuracy: 0.7460\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2997 - accuracy: 0.8966 - val_loss: 0.4698 - val_accuracy: 0.7302\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2867 - accuracy: 0.8966 - val_loss: 0.4810 - val_accuracy: 0.7778\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2805 - accuracy: 0.9172 - val_loss: 0.4666 - val_accuracy: 0.7619\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.2796 - accuracy: 0.8897 - val_loss: 0.4671 - val_accuracy: 0.7619\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.2678 - accuracy: 0.9241 - val_loss: 0.4695 - val_accuracy: 0.7460\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2590 - accuracy: 0.9172 - val_loss: 0.4841 - val_accuracy: 0.7778\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2486 - accuracy: 0.9103 - val_loss: 0.4691 - val_accuracy: 0.7460\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2403 - accuracy: 0.9310 - val_loss: 0.4658 - val_accuracy: 0.7460\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2407 - accuracy: 0.9241 - val_loss: 0.4723 - val_accuracy: 0.7302\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.2477 - accuracy: 0.9103 - val_loss: 0.4683 - val_accuracy: 0.7778\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2742 - accuracy: 0.8897 - val_loss: 0.4663 - val_accuracy: 0.7778\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2275 - accuracy: 0.9172 - val_loss: 0.4952 - val_accuracy: 0.7460\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2582 - accuracy: 0.8966 - val_loss: 0.4726 - val_accuracy: 0.7460\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2168 - accuracy: 0.9241 - val_loss: 0.4910 - val_accuracy: 0.7778\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.2134 - accuracy: 0.9172 - val_loss: 0.4802 - val_accuracy: 0.7778\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.2055 - accuracy: 0.9310 - val_loss: 0.4794 - val_accuracy: 0.7619\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.2087 - accuracy: 0.9241 - val_loss: 0.4959 - val_accuracy: 0.7619\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.2069 - accuracy: 0.9103 - val_loss: 0.5091 - val_accuracy: 0.7778\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1969 - accuracy: 0.9379 - val_loss: 0.4772 - val_accuracy: 0.7619\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1927 - accuracy: 0.9379 - val_loss: 0.4951 - val_accuracy: 0.7619\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1909 - accuracy: 0.9379 - val_loss: 0.5071 - val_accuracy: 0.7778\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1840 - accuracy: 0.9310 - val_loss: 0.4741 - val_accuracy: 0.7619\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1837 - accuracy: 0.9517 - val_loss: 0.4859 - val_accuracy: 0.7619\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1752 - accuracy: 0.9448 - val_loss: 0.5116 - val_accuracy: 0.7460\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1809 - accuracy: 0.9379 - val_loss: 0.5075 - val_accuracy: 0.7460\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1775 - accuracy: 0.9379 - val_loss: 0.4833 - val_accuracy: 0.7778\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1699 - accuracy: 0.9517 - val_loss: 0.4841 - val_accuracy: 0.7937\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1646 - accuracy: 0.9448 - val_loss: 0.4901 - val_accuracy: 0.7619\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1681 - accuracy: 0.9310 - val_loss: 0.4812 - val_accuracy: 0.7778\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1812 - accuracy: 0.9379 - val_loss: 0.5007 - val_accuracy: 0.7778\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1602 - accuracy: 0.9586 - val_loss: 0.4843 - val_accuracy: 0.7778\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1540 - accuracy: 0.9517 - val_loss: 0.4854 - val_accuracy: 0.7937\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1442 - accuracy: 0.9586 - val_loss: 0.4896 - val_accuracy: 0.7937\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1448 - accuracy: 0.9655 - val_loss: 0.4936 - val_accuracy: 0.7937\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9379 - val_loss: 0.4828 - val_accuracy: 0.7778\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1421 - accuracy: 0.9586 - val_loss: 0.4932 - val_accuracy: 0.7937\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1439 - accuracy: 0.9655 - val_loss: 0.4778 - val_accuracy: 0.7937\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1351 - accuracy: 0.9517 - val_loss: 0.4936 - val_accuracy: 0.7778\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1247 - accuracy: 0.9724 - val_loss: 0.5103 - val_accuracy: 0.7619\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1271 - accuracy: 0.9655 - val_loss: 0.4952 - val_accuracy: 0.7937\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1209 - accuracy: 0.9724 - val_loss: 0.5061 - val_accuracy: 0.7619\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1189 - accuracy: 0.9724 - val_loss: 0.5073 - val_accuracy: 0.7937\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.1134 - accuracy: 0.9793 - val_loss: 0.5000 - val_accuracy: 0.7937\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1117 - accuracy: 0.9793 - val_loss: 0.5118 - val_accuracy: 0.7619\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.1111 - accuracy: 0.9862 - val_loss: 0.5098 - val_accuracy: 0.7778\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1119 - accuracy: 0.9724 - val_loss: 0.5083 - val_accuracy: 0.7937\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1033 - accuracy: 0.9793 - val_loss: 0.5081 - val_accuracy: 0.7937\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1011 - accuracy: 0.9793 - val_loss: 0.5101 - val_accuracy: 0.7937\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1062 - accuracy: 0.9724 - val_loss: 0.5152 - val_accuracy: 0.7619\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1045 - accuracy: 0.9931 - val_loss: 0.5375 - val_accuracy: 0.7778\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1077 - accuracy: 0.9793 - val_loss: 0.5373 - val_accuracy: 0.7619\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0944 - accuracy: 0.9862 - val_loss: 0.5287 - val_accuracy: 0.7778\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0881 - accuracy: 0.9862 - val_loss: 0.5273 - val_accuracy: 0.7619\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0873 - accuracy: 0.9862 - val_loss: 0.5195 - val_accuracy: 0.7619\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0827 - accuracy: 0.9931 - val_loss: 0.5240 - val_accuracy: 0.7937\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0837 - accuracy: 0.9862 - val_loss: 0.5384 - val_accuracy: 0.7778\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0861 - accuracy: 0.9862 - val_loss: 0.5327 - val_accuracy: 0.7778\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0911 - accuracy: 0.9931 - val_loss: 0.5489 - val_accuracy: 0.7619\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0807 - accuracy: 0.9931 - val_loss: 0.5367 - val_accuracy: 0.7619\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9862 - val_loss: 0.5193 - val_accuracy: 0.7778\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0734 - accuracy: 0.9931 - val_loss: 0.5259 - val_accuracy: 0.7778\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0724 - accuracy: 0.9931 - val_loss: 0.5315 - val_accuracy: 0.7778\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0726 - accuracy: 0.9862 - val_loss: 0.5516 - val_accuracy: 0.7778\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0651 - accuracy: 0.9931 - val_loss: 0.5308 - val_accuracy: 0.7937\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0767 - accuracy: 0.9931 - val_loss: 0.5297 - val_accuracy: 0.7937\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0647 - accuracy: 0.9931 - val_loss: 0.5432 - val_accuracy: 0.7778\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0671 - accuracy: 0.9862 - val_loss: 0.5360 - val_accuracy: 0.7778\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0641 - accuracy: 0.9931 - val_loss: 0.5440 - val_accuracy: 0.7778\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 0.9931 - val_loss: 0.5583 - val_accuracy: 0.7778\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0550 - accuracy: 0.9931 - val_loss: 0.5504 - val_accuracy: 0.7619\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9931 - val_loss: 0.5568 - val_accuracy: 0.7619\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9862 - val_loss: 0.5819 - val_accuracy: 0.7778\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0522 - accuracy: 0.9931 - val_loss: 0.5630 - val_accuracy: 0.7778\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.99 - 0s 12ms/step - loss: 0.0528 - accuracy: 0.9931 - val_loss: 0.5563 - val_accuracy: 0.7619\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0530 - accuracy: 0.9931 - val_loss: 0.5690 - val_accuracy: 0.7619\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9931 - val_loss: 0.5593 - val_accuracy: 0.7778\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0468 - accuracy: 0.9931 - val_loss: 0.5712 - val_accuracy: 0.7778\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0451 - accuracy: 0.9931 - val_loss: 0.5587 - val_accuracy: 0.7619\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0430 - accuracy: 0.9931 - val_loss: 0.5718 - val_accuracy: 0.7778\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0452 - accuracy: 0.9931 - val_loss: 0.5956 - val_accuracy: 0.7778\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0418 - accuracy: 0.9931 - val_loss: 0.5726 - val_accuracy: 0.7619\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0399 - accuracy: 0.9931 - val_loss: 0.5768 - val_accuracy: 0.7619\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0396 - accuracy: 0.9931 - val_loss: 0.5870 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0414 - accuracy: 0.9931 - val_loss: 0.6084 - val_accuracy: 0.7778\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0449 - accuracy: 0.9931 - val_loss: 0.5786 - val_accuracy: 0.7778\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0392 - accuracy: 0.9931 - val_loss: 0.5862 - val_accuracy: 0.7619\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0352 - accuracy: 0.9931 - val_loss: 0.6163 - val_accuracy: 0.7778\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0365 - accuracy: 0.9931 - val_loss: 0.6372 - val_accuracy: 0.7778\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9931 - val_loss: 0.6323 - val_accuracy: 0.7778\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0361 - accuracy: 0.9931 - val_loss: 0.6161 - val_accuracy: 0.7778\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0314 - accuracy: 0.9931 - val_loss: 0.5952 - val_accuracy: 0.7778\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0299 - accuracy: 0.9931 - val_loss: 0.5965 - val_accuracy: 0.7778\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0295 - accuracy: 0.9931 - val_loss: 0.6167 - val_accuracy: 0.7778\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0283 - accuracy: 0.9931 - val_loss: 0.6067 - val_accuracy: 0.7778\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9931 - val_loss: 0.6014 - val_accuracy: 0.7778\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0272 - accuracy: 0.9931 - val_loss: 0.6263 - val_accuracy: 0.7778\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0255 - accuracy: 0.9931 - val_loss: 0.6099 - val_accuracy: 0.7937\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9931 - val_loss: 0.6047 - val_accuracy: 0.8095\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0251 - accuracy: 0.9931 - val_loss: 0.6291 - val_accuracy: 0.7778\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0258 - accuracy: 0.9931 - val_loss: 0.6250 - val_accuracy: 0.7778\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.6286 - val_accuracy: 0.7778\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.6366 - val_accuracy: 0.7778\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 0.7778\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.6410 - val_accuracy: 0.7778\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 0.9931 - val_loss: 0.6468 - val_accuracy: 0.7778\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.6576 - val_accuracy: 0.7778\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.6521 - val_accuracy: 0.7778\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 0.7778\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.6692 - val_accuracy: 0.7778\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.7778\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.6657 - val_accuracy: 0.7778\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.6640 - val_accuracy: 0.7778\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.7778\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.6712 - val_accuracy: 0.7778\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 0.7778\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.6800 - val_accuracy: 0.7778\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.7937\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.7937\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6967 - val_accuracy: 0.7778\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 0.7937\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.7778\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.6945 - val_accuracy: 0.7778\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.7937\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.7778\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6986 - val_accuracy: 0.7778\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.6957 - val_accuracy: 0.7937\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6856 - val_accuracy: 0.7937\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.7024 - val_accuracy: 0.7778\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.7104 - val_accuracy: 0.7778\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.7090 - val_accuracy: 0.7778\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.7115 - val_accuracy: 0.7778\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.7419 - val_accuracy: 0.7778\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.7228 - val_accuracy: 0.7778\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.7430 - val_accuracy: 0.7778\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.7494 - val_accuracy: 0.7778\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.7073 - val_accuracy: 0.7937\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.7778\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.7491 - val_accuracy: 0.7778\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.7224 - val_accuracy: 0.7937\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.7361 - val_accuracy: 0.7937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.7800 - val_accuracy: 0.7778\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.7937\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.7532 - val_accuracy: 0.7778\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.7929 - val_accuracy: 0.7778\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.7778\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.7814 - val_accuracy: 0.7778\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.7778\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.7584 - val_accuracy: 0.7937\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.7678 - val_accuracy: 0.7778\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7860 - val_accuracy: 0.7778\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7663 - val_accuracy: 0.7937\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.7820 - val_accuracy: 0.7937\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.7797 - val_accuracy: 0.7937\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.8064 - val_accuracy: 0.7778\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.7701 - val_accuracy: 0.7937\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.8038 - val_accuracy: 0.7778\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7803 - val_accuracy: 0.7937\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.7957 - val_accuracy: 0.7937\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.8310 - val_accuracy: 0.7778\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.7806 - val_accuracy: 0.7937\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.7989 - val_accuracy: 0.7937\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8433 - val_accuracy: 0.7778\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.7937\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7958 - val_accuracy: 0.7937\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8066 - val_accuracy: 0.7937\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8369 - val_accuracy: 0.7778\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8430 - val_accuracy: 0.7778\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8609 - val_accuracy: 0.7778\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.8133 - val_accuracy: 0.7937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2693a0fe610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_data : 검증용 데이터를 설정한다.\n",
    "model.fit(X_train, y_train,validation_data=(X_test, y_test), epochs=200, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
