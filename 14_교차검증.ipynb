{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 경고 뜨지 않게 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 데이터 전처리 알고리즘\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습용과 검증용으로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 교차 검증\n",
    "# 지표를 하나만 설정할 경우\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 지표를 하나 이상 설정할 경우\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 모델의 최적의 하이퍼파라미터를 찾기 위한 도구\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 평가함수\n",
    "# 분류용\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 회귀용\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 군집화\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "\n",
    "\n",
    "\n",
    "# ARIMA (시계열 예측)\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# 시간 측정을 위한 시간 모듈\n",
    "import datetime\n",
    "\n",
    "# 주식정보\n",
    "from pandas_datareader import data\n",
    "\n",
    "# 형태소 벡터를 생성하기 위한 라이브러리\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 형태소 벡터를 학습 벡터로 변환한다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# 데이터 수집\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 한국어 형태소 분석\n",
    "from konlpy.tag import Okt, Hannanum, Kkma, Mecab, Komoran\n",
    "\n",
    "# 워드 클라우드를 위한 라이브러리\n",
    "from collections import Counter\n",
    "import pytagcloud\n",
    "from IPython.display import Image\n",
    "\n",
    "# 출력창 청소를 위한 함수\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "\n",
    "# 딥러닝 모델 구조를 정의하는 것\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 층구조를 정의하는 것\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 활성화 함수를 정의하는 것\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# 다중 분류를 위한 원핫 인코딩\n",
    "# 결과 데이터의 종류 수 만큼 결과 데이터의 칼럼을 늘리는 작업\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 현재 프로젝트를 gpu에 할당한다.\n",
    "# 컴퓨터의 GPU는 메모리를 가지고 있다. \n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# gpu가 있다면\n",
    "if len(gpus) >0 :\n",
    "    try :\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e :\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/sonar.csv\", header=None)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n",
       "1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
       "2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
       "3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
       "4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
       "204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n",
       "205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n",
       "206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n",
       "207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n",
       "\n",
       "         57      58      59  \n",
       "0    0.0084  0.0090  0.0032  \n",
       "1    0.0049  0.0052  0.0044  \n",
       "2    0.0164  0.0095  0.0078  \n",
       "3    0.0044  0.0040  0.0117  \n",
       "4    0.0048  0.0107  0.0094  \n",
       "..      ...     ...     ...  \n",
       "203  0.0115  0.0193  0.0157  \n",
       "204  0.0032  0.0062  0.0067  \n",
       "205  0.0138  0.0077  0.0031  \n",
       "206  0.0079  0.0036  0.0048  \n",
       "207  0.0036  0.0061  0.0115  \n",
       "\n",
       "[208 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n",
       "1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
       "2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
       "3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
       "4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
       "204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n",
       "205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n",
       "206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n",
       "207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n",
       "\n",
       "         57      58      59  \n",
       "0    0.0084  0.0090  0.0032  \n",
       "1    0.0049  0.0052  0.0044  \n",
       "2    0.0164  0.0095  0.0078  \n",
       "3    0.0044  0.0040  0.0117  \n",
       "4    0.0048  0.0107  0.0094  \n",
       "..      ...     ...     ...  \n",
       "203  0.0115  0.0193  0.0157  \n",
       "204  0.0032  0.0062  0.0067  \n",
       "205  0.0138  0.0077  0.0031  \n",
       "206  0.0079  0.0036  0.0048  \n",
       "207  0.0036  0.0061  0.0115  \n",
       "\n",
       "[208 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df1.drop(60, axis=1)\n",
    "y = df1[60]\n",
    "\n",
    "display(X)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(y)\n",
    "y = encoder1.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOld 생성\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 0.6385 - accuracy: 0.8571\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.6464 - accuracy: 0.7619\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.1284 - accuracy: 0.8095\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 1.0727 - accuracy: 0.8095\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000002AEF9E31E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.1470 - accuracy: 0.9048\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x000002AEAB029700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.7185 - accuracy: 0.8571\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.7402 - accuracy: 0.8095\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.6235 - accuracy: 0.8095\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.1001 - accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.9279 - accuracy: 0.8500\n",
      "[0.8571428656578064, 0.761904776096344, 0.8095238208770752, 0.8095238208770752, 0.9047619104385376, 0.8571428656578064, 0.8095238208770752, 0.8095238208770752, 0.8999999761581421, 0.8500000238418579]\n"
     ]
    }
   ],
   "source": [
    "# 예측 정확도를 담을 리스트\n",
    "result_list = []\n",
    "\n",
    "# 폴드의 수만큼 반복한다.\n",
    "# 폴드의 수 만큼 반복한다.\n",
    "for train_idx, test_idx in kfold.split(X) :\n",
    "    # 모델 설정\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(24, input_dim=60))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # 학습한다.\n",
    "    model.fit(X.loc[train_idx], y[train_idx], epochs=200, batch_size=5, verbose=0)\n",
    "    \n",
    "    #  검증\n",
    "    r1 = model.evaluate(X.loc[test_idx], y[test_idx])\n",
    "    result_list.append(r1[1])\n",
    "    \n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 성능 그래프\n",
    "plt.plot(result_list)\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
